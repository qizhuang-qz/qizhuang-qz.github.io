<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="icon" type="image/x-icon" href="images/qizhuang.png" />
<title>Zhuang Qi (é½å£®)</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Zhuang Qi (é½å£®) </h1>
</div>
<table class="imgtable"><tr>
<!-- ç¬¬ä¸€åˆ— -->
<td><a href="https://qizhuang-qz.github.io/"><img src="images/qizhuang.png" alt="alt text" width="135px" /></a>&nbsp;</td>
<!-- ç¬¬äºŒåˆ— -->
<td align="left">
<p>æˆ‘ç›®å‰åœ¨<a href="https://www.sdu.edu.cn/">å±±ä¸œå¤§å­¦</a><a href="https://www.sc.sdu.edu.cn/">è½¯ä»¶å­¦é™¢</a>æ”»è¯»åšå£«å­¦ä½ï¼Œå¸ˆä»<a href="https://ercdm.sdu.edu.cn/info/1013/1523.htm"><b>å­Ÿé›·æ•™æˆ</b></a>ã€‚æœ¬äººå·²äº2022å¹´6æœˆä»æ±•å¤´å¤§å­¦å·¥å­¦é™¢è®¡ç®—æœºç³»è·å¾—å·¥ç¨‹ç¡•å£«å­¦ä½ï¼›äº2019å¹´6æœˆåœ¨æ²³åŒ—ç§‘æŠ€å¤§å­¦ç†å­¦é™¢ï¼Œä¿¡æ¯ä¸è®¡ç®—ç§‘å­¦ä¸“ä¸šè·å¾—äº†ç†å­¦å­¦å£«å­¦ä½ã€‚åœ¨å­¦æœŸé—´ï¼Œå›´ç»•äº’è”ç½‘å¤§æ•°æ®é©±åŠ¨ä¸‹çš„å¤šåª’ä½“è®¡ç®—å’Œæ•°æ®æŒ–æ˜ç­‰ç§‘å­¦é—®é¢˜å¼€å±•ç ”ç©¶ï¼Œä¸»è¦å†…å®¹åŒ…æ‹¬ç¦»ç¾¤å€¼æ£€æµ‹ç®—æ³•ç ”ç©¶ä»¥åŠå¤šæºä¿¡æ¯èåˆçš„è”é‚¦å­¦ä¹ ç®—æ³•ç ”ç©¶ã€‚ç›¸å…³æˆæœå·²åœ¨ACM MMã€ECCVã€ICMEã€TNNLSã€IDAJç­‰å›½å†…å¤–æƒå¨ä¼šè®®å’ŒæœŸåˆŠä¸­å‘è¡¨ã€‚æ­¤å¤–ï¼Œæˆ‘å°†äº2024å¹´12æœˆä»¥è”åˆåŸ¹å…»åšå£«ç”Ÿèº«ä»½å‰å¾€æ–°åŠ å¡å—æ´‹ç†å·¥å¤§å­¦è®¡ç®—ä¸æ•°æ®ç§‘å­¦å­¦é™¢<a href="https://personal.ntu.edu.sg/han.yu/"><b>Han Yuæ•™æˆå›¢é˜Ÿ</b></a>äº¤æµå­¦ä¹ ã€‚</p>
<p>æˆ‘çš„ç ”ç©¶å…´è¶£ä¸»è¦åŒ…æ‹¬ğŸ˜ğŸ˜ğŸ˜: <br />
âœ”ï¸ <b>è”é‚¦å­¦ä¹  (Federated Learning)</b><br />
âœ”ï¸ <b>åˆ†å¸ƒå¤–æ³›åŒ– (Out-of-Distribution Generalization) </b><br />  
âœ”ï¸ <b>å¼‚å¸¸å€¼æ£€æµ‹ (Outlier Detection) </b><br /> 
</td>
<!-- ç¬¬ä¸‰åˆ— -->
<td align="left" class="contact-info">
  <ul><li><a href="mailto:z_qi@mail.sdu.edu.cn">ğŸ“§  [E-mail]</a></li></ul>
  <ul><li><a href="https://scholar.google.com/citations?user=l7wxjVwAAAAJ&hl=zh-CN">ğŸ“ [Google Scholar]</a></li></ul>
  <ul><li><a href="https://github.com/qizhuang-qz">ğŸ“¥ [GitHub]</a></li></ul>
  <ul><li><a href="https://orcid.org/0000-0002-0656-2309">ğŸ†” [ORCID]</a></li></ul>
  <ul><li><a href="EnHome.html">ğŸ“‘ [English Page]</a></li></ul>
</td>
</tr></table>

<h2>æ•™è‚²ç»å†</h2> 
<ul>
<li><p>2024.12 ~ ç°åœ¨: æ–°åŠ å¡å—æ´‹ç†å·¥å¤§å­¦, è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢, è®¿é—®åšå£« (CSC) [å¯¼å¸ˆ: ğŸ’â€â™‚ï¸<a href="https://personal.ntu.edu.sg/han.yu/">Han Yuæ•™æˆ</a>ğŸ‘] </p></li>
<li><p>2022.09 ~ ç°åœ¨: å±±ä¸œå¤§å­¦, è½¯ä»¶å­¦é™¢, åœ¨è¯»åšå£« [å¯¼å¸ˆ: ğŸ’â€â™‚ï¸<a href="https://ercdm.sdu.edu.cn/info/1013/1523.htm">å­Ÿé›·æ•™æˆ</a>ğŸ‘]</p></li>
<li><p>2019.09 ~ 2022.07: æ±•å¤´å¤§å­¦, å·¥å­¦é™¢è®¡ç®—æœºç³», ç¡•å£« </p></li>
<li><p>2015.09 ~ 2019.07: æ²³åŒ—ç§‘æŠ€å¤§å­¦, ç†å­¦é™¢, å­¦å£« </p></li>
</ul>

<h2>å­¦æœ¯è®ºæ–‡(*è¡¨ç¤ºé€šè®¯ä½œè€…)</h2>
  <h4>2025</h4>
    <ul><li><p><a href="https://ieeexplore.ieee.org/abstract/document/10605121/">Cross-Silo Feature Space Alignment for Federated Learning on Clients with Imbalanced Data</a> <br />
    <b>Zhuang Qi</b>, Lei Meng*, Zhaochuan Li, Han Hu, Xiangxu Meng <br />
    <i>The Annual AAAI Conference on Artificial Intelligence 2025 (<b>AAAI'25</b>), 2025. </i> <br /> 
    <span class="special">[CCF Aç±»ä¼šè®®]</span> <br /> 
    </p></li></ul>
  
    <ul><li><p><a href="https://ieeexplore.ieee.org/abstract/document/10605121/">Causal Inference over Visual-Semantic-Aligned Graph for Image Classification</a> <br />
    Lei Meng, Xiangxian Li, Xiaoshuo Yan*, Haokai Ma, <b>Zhuang Qi</b>, Wei Wu, Xiangxu Meng <br />
    <i>The Annual AAAI Conference on Artificial Intelligence 2025 (<b>AAAI'25</b>), 2025. </i> <br /> 
    <span class="special">[CCF Aç±»ä¼šè®®]</span> <br /> 
    </p></li></ul>
  
  <h4>2024</h4>
    <ul><li><p><a href="https://ieeexplore.ieee.org/abstract/document/10605121/">Improving Global Generalization and Local Personalization for Federated Learning</a> <br />
    Lei Meng, <b>Zhuang Qi*</b>, Lei Wu, Xiaoyu Du, Zhaochuan Li, Lizhen Cui, Xiangxu Meng <br />
    <i>IEEE Transactions on Neural Networks and Learning Systems (<b>TNNLS'24</b>), 2024. </i> <br /> 
    <span class="special">[ä¸­ç§‘é™¢ä¸€åŒº]</span> <br /> 
    </p></li></ul>
  
    <ul><li><p><a href="https://ercdm.sdu.edu.cn/__local/C/E9/9D/A1BD5A5C57CE80961D6A99019C0_AF56C445_85AE4.pdf">Attentive Modeling and Distillation for Out-of-Distribution Generalization of Federated Learning</a> <br />
    <b>Zhuang Qi</b>, Weihao He, Xiangxu Meng, Lei Meng <br />
    <i>IEEE International Conference on Multimedia and Expo (<b>ICME'24</b>), 2024. </i> <br /> 
    <span class="special">[CCF Bç±»ä¼šè®®]</span><br /> 
    </p></li></ul>
  
    <ul><li><p><a href="https://link.springer.com/article/10.1007/s41095-023-0382-0">Cross-modal learning using privileged information for long-tailed image classification</a> <br />
    Xiangxian Li, Yuze Zheng, Haokai Ma, <b>Zhuang Qi</b>, Xiangxu Meng, Lei Meng <br />
    <i>Computational Visual Media (<b>CVM'24</b>), 2024. </i> <br /> 
    <span class="special">[ä¸­ç§‘é™¢äºŒåŒº, CCF Cç±»æœŸåˆŠ]</span> <br /> 
    </p></li></ul>
  
    <ul><li><p><a href="https://arxiv.org/abs/2405.20046">Cross-Training with Multi-View Knowledge Fusion for Heterogenous Federated Learning</a> <br />
    <b>Zhuang Qi</b>, Lei Meng, Weihao He, Ruohan Zhang, Yu Wang, Xin Qi, Xiangxu Meng <br />
    <i>Arxiv, 2024. </i> <br /> 
    </p></li></ul>
      
    <ul><li><p><a href="https://www.mdpi.com/1420-3049/29/4/903">Machine Learning Empowering Drug Discovery: Applications, Opportunities and Challenges</a> <br /> 
    Xin Qi, Yuanchun Zhao, <b>Zhuang Qi</b>, Siyu Hou, Jiajia Chen <br />
    <i>Molecules, 2024. </i> <br /> 
    <span class="special">[ä¸­ç§‘é™¢äºŒåŒº]</span> <br /> 
    </p></li></ul>

    <ul><li><p><a href="https://www.jos.org.cn/jos/article/abstract/7052">åŸºäºè·¨æ¨¡æ€ç‰¹æƒä¿¡æ¯å¢å¼ºçš„å›¾åƒåˆ†ç±»æ–¹æ³•</a> <br /> 
    æè±¡è´¤ï¼Œéƒ‘è£•æ³½ï¼Œé©¬æµ©å‡¯ï¼Œ<b>é½å£®</b>ï¼Œé—«æ™“ç¡•ï¼Œå­Ÿç¥¥æ—­ï¼Œå­Ÿé›· <br />
    <i>è½¯ä»¶å­¦æŠ¥, 2024. </i> <br /> 
    <span class="special">[CCF Aç±»ä¸­æ–‡æœŸåˆŠ]</span> <br /> 
    </p></li></ul>

  <h4>2023</h4>
    <ul><li><p><a href="https://dl.acm.org/doi/abs/10.1145/3595916.3626390">Class-aware convolution and attentive aggregation for image classification</a> <br />
    Zitan Chen, <b>Zhuang Qi</b>, Xiangxian Li, Yuqing Wang, Lei Meng, Xiangxu Meng <br />
    <i>Proceedings of the 5th ACM International Conference on Multimedia in Asia (<b>MM Asia'23</b>), 2023. </i> <br /> 
    <span class="special">[CCF Cç±»ä¼šè®®]</span><br /> 
    </p></li></ul>

    <ul><li><p><a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612511">Class-level Structural Relation Modeling and Smoothing for Visual Representation Learning</a> <br />
    Zitan Chen, <b>Zhuang Qi</b>, Xiao Cao, Xiangxian Li, Xiangxu Meng, Lei Meng <br />
    <i>Proceedings of the 31st ACM International Conference on Multimedia (<b>MM'23</b>), 2023. </i> <br /> 
    <span class="special">[CCF Cç±»ä¼šè®®]</span><br /> 
    </p></li></ul>

    <ul><li><p><a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612481">Cross-silo prototypical calibration for federated learning with non-iid data</a> <br />
    <b>Zhuang Qi</b>, Lei Meng, Zitan Chen, Han Hu, Hui Lin, Xiangxu Meng <br />
    <i>Proceedings of the 31st ACM International Conference on Multimedia (<b>MM'23</b>), 2023. </i> <br />
    <span class="special">[CCF Aç±»ä¼šè®®]</span><br /> 
    </p></li></ul>
  
    <ul><li><p><a href="https://link.springer.com/chapter/10.1007/978-3-031-46314-3_28">Unsupervised Segmentation of Haze Regions as Hard Attention for Haze Classification</a> <br />
    Jingyu Li, Haokai Ma, Xiangxian Li, <b>Zhuang Qi</b>, Xiangxu Meng, Lei Meng <br />
    <i>International Conference on Image and Graphics (<b>ICIG'23</b>), 2023. </i> <br /> 
    <span class="special">[EIä¼šè®®]</span><br /> 
    </p></li></ul>
   
    <ul><li><p><a href="https://link.springer.com/chapter/10.1007/978-3-031-46314-3_29">Learning to fuse residual and conditional information for video compression and reconstruction</a> <br />
    Ran Wang, <b>Zhuang Qi</b>, Xiangxu Meng, Lei Meng <br />
    <i>International Conference on Image and Graphics (<b>ICIG'23</b>), 2023. </i> <br /> 
    <span class="special">[EIä¼šè®®]</span> <br /> 
    </p></li></ul>
  
    <ul><li><p><a href="https://ieeexplore.ieee.org/abstract/document/10220061">Cross-Training with Prototypical Distillation for improving the generalization of Federated Learning</a> <br />
    Tianhan Liu, <b>Zhuang Qi</b>, Zitan Chen, Xiangxu Meng, Lei Meng <br />
    <i>2023 IEEE International Conference on Multimedia and Expo (<b>ICME'23</b>), 2023. </i> <br /> 
    <span class="special">[CCF Bç±»ä¼šè®®]</span> <br /> 
    </p></li></ul>

    <ul><li><p><a href="https://ieeexplore.ieee.org/abstract/document/10191979/">Cross-modal content inference and feature enrichment for cold-start recommendation</a> <br />
    Haokai Ma, <b>Zhuang Qi</b>, Xinxin Dong, Xiangxian Li, Yuze Zheng, Xiangxu Meng, Lei Meng <br />
    <i>2023 International Joint Conference on Neural Networks (<b>IJCNN'23</b>), 2023. </i> <br /> 
    <span class="special">[CCF Cç±»ä¼šè®®]</span> <br /> 
    </p></li></ul>

    <ul><li><p><a href="https://ieeexplore.ieee.org/abstract/document/10192036">Multi-channel attentive weighting of visual frames for multimodal video classification</a> <br />
    Yuqing Wang, <b>Zhuang Qi</b>, Xiangxian Li, Jinxing Liu, Xiangxu Meng, Lei Meng <br />
    <i>2023 International Joint Conference on Neural Networks (<b>IJCNN'23</b>), 2023. </i> <br /> 
    <span class="special">[CCF Cç±»ä¼šè®®]</span> <br /> 
    </p></li></ul>

  <h4>2022</h4>
    <ul><li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025521013402">Meta-causal feature learning for out-of-distribution generalization</a> <br />
    Yuqing Wang, Xiangxian Li, <b>Zhuang Qi</b>, Jingyu Li, Xuelong Li, Xiangxu Meng, Lei Meng <br />
    <i>European Conference on Computer Vision (<b>ECCV'22</b>), 2022. </i> <br /> 
    <span class="special">[CCF Bç±»ä¼šè®®]</span> <br /> 
    </p></li></ul>

    <ul><li><p><a href="https://link.springer.com/chapter/10.1007/978-3-031-25075-0_36">Causal inference with sample balancing for out-of-distribution detection in visual classification</a> <br />
    Yuqing Wang, Xiangxian Li, Haokai Ma, <b>Zhuang Qi</b>, Xiangxu Meng, Lei Meng <br />
    <i>CAAI International Conference on Artificial Intelligence (<b>CICAI'22</b>), 2022. </i> <br /> 
    <span class="special">[CAAI Aç±»ä¼šè®®]</span><br /> 
    </p></li></ul>

    <ul><li><p><a href="https://link.springer.com/chapter/10.1007/978-3-031-20503-3_13">Clustering-based curriculum construction for sample-balanced federated learning</a> <br />
    <b>Zhuang Qi</b>, Yuqing Wang, Zitan Chen, Ran Wang, Xiangxu Meng, Lei Meng <br />
    <i>CAAI International Conference on Artificial Intelligence (<b>CICAI'22</b>), 2022. </i> <br /> 
    <span class="special">[CAAI Aç±»ä¼šè®®]</span> <br /> 
    </p></li></ul>

    <ul><li><p><a href="https://link.springer.com/chapter/10.1007/978-3-031-20497-5_34">Prompt learning with cross-modal feature alignment for visual domain adaptation</a> <br />
    Jinxing Liu, Junjin Xiao, Haokai Ma, Xiangxian Li, <b>Zhuang Qi</b>, Xiangxu Meng, Lei Meng <br />
    <i>CAAI International Conference on Artificial Intelligence (<b>CICAI'22</b>), 2022. </i> <br /> 
    <span class="special">[CAAI Aç±»ä¼šè®®]</span><br /> 
    </p></li></ul>

    <ul><li><p><a href="https://dl.acm.org/doi/abs/10.1145/3512527.3531370">Unsupervised contrastive masking for visual haze classification</a> <br />
    Jingyu Li, Haokai Ma, Xiangxian Li, <b>Zhuang Qi</b>, Lei Meng, Xiangxu Meng <br />
    <i>Proceedings of the 2022 International Conference on Multimedia Retrieval (<b>ICMR'22</b>), 2022. </i> <br /> 
    <span class="special">[CCF Bç±»ä¼šè®®]</span> <br /> 
    </p></li></ul>

    <ul><li><p><a href="https://arxiv.org/abs/2406.00589">Robust Visual Tracking via Iterative Gradient Descent and Threshold Selectiong</a> <br />
    <b>Zhuang Qi</b>, Junlin Zhang, Xin Qi <br /> 
    <i>Arxiv, 2022. </i> <br />
    </p></li></ul>
  
    <ul><li><p><a href="https://content.iospress.com/articles/intelligent-data-analysis/ida216257">A novel density-based outlier detection method using key attributes</a> <br />
    <b>Zhuang Qi</b>, Xiaming Chen <br />
    <i>Intelligent Data Analysis Journal (<b>IDA'22</b>), 2022. </i> <br /> 
    <span class="special">[CCF Cç±»æœŸåˆŠ]</span> <br /> 
    </p></li></ul>
  
  <h4>2021</h4>
    <ul><li><p><a href="https://www.worldscientific.com/doi/abs/10.1142/S0219691321500041">Iterative gradient descent for outlier detection</a> <br />
    <b>Zhuang Qi</b>, Dazhi Jiang, Xiaming Chen <br />
    <i>International Journal of Wavelets, Multiresolution and Information Processing (<b>IJWMIP'21</b>), 2021. </i> <br /> 
    <span class="special">[ä¸­ç§‘é™¢å››åŒº]</span> <br /> 
    </p></li></ul>

    <ul><li><p><a href="https://arxiv.org/abs/2405.19606">Relation Modeling and Distillation for Learning with Noisy Labels</a> <br />
    <b>Zhuang Qi</b>, Junlin Zhang, Xiaming Chen*, Xin Qi* <br />
    <i>Arxiv, 2021. </i> <br /> 
    </p></li></ul>
    
    <ul><li><p><a href="https://arxiv.org/abs/2405.19247">Comparative Study of Neighbor-based Methods for Local Outlier Detection</a> <br />
    <b>Zhuang Qi</b>, Junlin Zhang, Xiaming Chen, Xin Qi <br />
    <i>Arxiv, 2021. </i> <br /> 
    </p></li></ul>
  
  <h4>2020</h4>
    <ul><li><p><a href="https://ieeexplore.ieee.org/abstract/document/9028237/">Hdiht: a high-accuracy distributed iterative hard thresholding algorithm for compressed sensing</a> <br />
    Xiaming Chen, <b>Zhuang Qi</b>, Jianlong Xu <br />
    <i>IEEE Access, 2020. </i> <br /> 
    <span class="special">[ä¸­ç§‘é™¢äºŒåŒº]</span> <br /> 
    </p></li></ul>

<h2>è£èª‰å¥–åŠ±</h2>
<ul>
<li><p>æ²³åŒ—ç§‘æŠ€å¤§å­¦2016-2017å­¦å¹´åº¦æ ¡çº§æ¨¡èŒƒå­¦ç”Ÿå¹²éƒ¨ (2017.11) </p></li>
<li><p>2017å¹´åº¦å›½å®¶åŠ±å¿—å¥–å­¦é‡‘ (2017.11) </p></li>
<li><p>æ²³åŒ—ç§‘æŠ€å¤§å­¦2016-2017å­¦å¹´åº¦æ ¡çº§ä¸‰ç­‰å¥–å­¦é‡‘ (2017.11) </p></li>
<li><p>2018å¹´æ²³åŒ—çœå¤§å­¦ç”Ÿæ•°å­¦ç«èµ›ä¸€ç­‰å¥– (2018.12) </p></li>
<li><p>2018å¹´å…¨å›½å¤§å­¦ç”Ÿæ•°å­¦ç«èµ›ä¸‰ç­‰å¥– (2018.12) </p></li>
<li><p>æ±•å¤´å¤§å­¦2021å¹´åº¦ç¡•å£«ç”Ÿä¸€ç­‰å­¦ä¸šå¥–å­¦é‡‘ (2021.12) </p></li>
<li><p>2021å¹´å…¨å›½é«˜ç­‰é™¢æ ¡æ•°å­¦èƒ½åŠ›æŒ‘æˆ˜èµ›å†³èµ›äºŒç­‰å¥– (2021.12) </p></li>
<li><p>2022å¹´é¦–å±ŠIPv6æŠ€æœ¯åº”ç”¨åˆ›æ–°å¤§èµ›ç§‘æ•™èµ›é“æš¨ç¬¬å…­å±Šä¸‹ä¸€ä»£äº’è”ç½‘æŠ€æœ¯åˆ›æ–°å¤§èµ›ä¸­è·å¾—å†³èµ›ä¼˜ç§€å¥– (2022.12)</p></li>
<li><p>ç¬¬åå››å±Šä¸­å›½å¤§å­¦ç”ŸæœåŠ¡å¤–åŒ…åˆ›æ–°åˆ›ä¸šå¤§èµ›â€œåˆåˆä¿¡æ¯æ¯â€åŒ—éƒ¨èµ›åŒºèµ›ï¼Œåˆ›æ–°å®è·µç±»ï¼ŒäºŒç­‰å¥– (2023.06)</p></li>
<li><p>ç¬¬åå››å±Šä¸­å›½å¤§å­¦ç”ŸæœåŠ¡å¤–åŒ…åˆ›æ–°åˆ›ä¸šå¤§èµ›ï¼Œåˆ›æ–°å®è·µç±»ï¼Œä¸‰ç­‰å¥– (2023.07)</p></li>
<li><p>ç¬¬å››å±ŠCCFå¤§å­¦ç”Ÿå­¦æœ¯ç§€ï¼Œåšå£«ç»„ï¼Œä¸‰ç­‰å¥– (2024.05)</p></li>
<li><p>2024å¹´å›½å®¶å…¬æ´¾ç•™å­¦èµ„æ ¼ï¼ˆCSCï¼‰ï¼Œè”åˆåŸ¹å…»åšå£«ç”Ÿ (2024.07)</p></li>
</ul>  

  
<h2>å‘æ˜ä¸“åˆ©</h2>
<ul>
<li><p>åŸºäºåŸå‹å¼•å¯¼äº¤å‰è®­ç»ƒæœºåˆ¶çš„è”é‚¦å­¦ä¹ ç³»ç»ŸåŠæ–¹æ³• <br />
å­Ÿé›·, <b>é½å£®</b>, åˆ˜å¤©æ¶µ, å­Ÿç¥¥æ—­<br />
å›½å®¶å‘æ˜ä¸“åˆ©, ç”³è¯·å·: 2023103105396, 2023.3.23.</p>

<li><p>åŸºäºè·¨æ¨¡æ€è¯­ä¹‰æ¨ç†å’Œèåˆçš„è§†è§‰æ„ŸçŸ¥æ¨èæ–¹æ³•åŠç³»ç»Ÿ <br />
å­Ÿé›·, é©¬æµ©å‡¯, <b>é½å£®</b>, æè±¡è´¤, éƒ‘è£•æ³½, å­Ÿç¥¥æ—­<br />
å›½å®¶å‘æ˜ä¸“åˆ©, æˆæƒå…¬å‘Šå·: CN 114936901 B, 2024.05.28.</p>
  
<li><p>åŸºäºè·¨æ¨¡æ€è¯­ä¹‰è¡¨å¾å­¦ä¹ å’Œèåˆçš„å›¾åƒåˆ†ç±»æ–¹æ³•åŠç³»ç»Ÿ <br />
å­Ÿé›·, æè±¡è´¤, éƒ‘è£•æ³½, é©¬æµ©å‡¯, <b>é½å£®</b>, å­Ÿç¥¥æ—­<br />
å›½å®¶å‘æ˜ä¸“åˆ©, æˆæƒå…¬å‘Šå·: CN 114898156 B, 2024.06.04.</p>
</ul>

<h2>è½¯ä»¶è‘—ä½œæƒ</h2>
<ul>
<li><p>ç¤¾ä¼šæ²»ç†æ•°å­—å­ªç”Ÿå®æ™¯ä¸‰ç»´ç³»ç»Ÿ <br />
è‘—ä½œæƒäººï¼šå±±ä¸œå¤§å­¦<br />
ç™»è®°å·ï¼š2023SR0716770.</p>

<li><p>ç¤¾ä¼šæ²»ç†å¤šæºæ•°æ®å¤„ç† <br />
è‘—ä½œæƒäººï¼šå±±ä¸œå¤§å­¦<br />
ç™»è®°å·ï¼š2023SR0887450.</p>
</ul>
  

<!-- <h2>è¯¾é¢˜é¡¹ç›®</h2> -->
<!-- <ul><li><p>é¢å‘éšç§ä¿æŠ¤æ•°æ®çš„è”é‚¦å› æœå…³ç³»æ¨æ–­ç®—æ³•ç ”ç©¶ (No.62376087), 2024.01-2027.12 <br /> -->
<!-- <b>é¡¹ç›®éª¨å¹²</b>;  ç»è´¹:51ä¸‡å…ƒ <br /> -->
<!-- å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘é¢ä¸Šé¡¹ç›® <br /></p></li></ul> -->

<!-- <ul><li><p>è·¨åª’ä½“å› æœæ¨æ–­ä¸å¯ä¿¡æœºå™¨å­¦ä¹  (No. 2021ZD0111801), 2021.12-2025.11 <br /> -->
<!-- <b>å­è¯¾é¢˜éª¨å¹²</b>;  ç»è´¹:180ä¸‡å…ƒ <br /> -->
<!-- ç§‘æŠ€éƒ¨ç§‘æŠ€åˆ›æ–°2030-â€œæ–°ä¸€ä»£äººå·¥æ™ºèƒ½â€é‡å¤§é¡¹ç›® <br /></p></li></ul> -->

<!-- <ul><li><p>å¸¸è¯†çŸ¥è¯†å­¦ä¹ ä¸å› æœåˆ†æ (No. 2020AAA0106100), 2020.11-2024.10 <br /> -->
<!-- <b>è¯¾é¢˜éª¨å¹²</b>;  ç»è´¹:180ä¸‡å…ƒ <br /> -->
<!-- ç§‘æŠ€éƒ¨ç§‘æŠ€åˆ›æ–°2030-â€œæ–°ä¸€ä»£äººå·¥æ™ºèƒ½â€é‡å¤§é¡¹ç›® <br /></p></li></ul> -->


<h2>å­¦æœ¯æœåŠ¡</h2>
<ul>
  <li><p>International Conference on Machine Learning 2025 (ICML'25), å®¡ç¨¿äºº </p></li>
  <li><p>The Annual AAAI Conference on Artificial Intelligence 2025 (AAAI'25), PC Member </p></li>
  <li><p>ACM International Conference on Multimedia 2023, 2024 (MM'23, MM'24), å®¡ç¨¿äºº </p></li>
  <li><p>IEEE Conference on Computer Vision and Pattern Recognition 2024,2025 (CVPR'24, CVPR'25), å®¡ç¨¿äºº </p></li>
  <li><p>IEEE International Conference on Multimedia and Expo 2024, 2025 (ICME'24, ICME'25), å®¡ç¨¿äºº </p></li>
  <li><p>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), å®¡ç¨¿äºº, 2024.08-è‡³ä»Š </p></li>
  <li><p>Computer Methods in Biomechanics and Biomedical Engineering, å®¡ç¨¿äºº, 2024.09-è‡³ä»Š </p></li>
  <li><p>IEEE Transactions on Industrial Informatics, å®¡ç¨¿äºº, 2025.01-è‡³ä»Š </p></li>
  
</ul>

<!-- <h2>ç½‘ç«™è®¿é—®ç»Ÿè®¡ (è‡ª2023å¹´12æœˆèµ·)</h2> -->
<!-- <a href="https://clustrmaps.com/site/1bxn1"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=4V6eYQ2IPE90W5uH_zCPr6SfzhrxmDDztM3gjnQ8ILE&cl=ffffff" /></a><br /> -->
<!-- <br /><br /> -->
<!-- <img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&pause=1000&color=119DF7&background=428FFF31&center=true&vCenter=true&random=false&width=435&lines=ç¥æ‚¨åœ¨å­¦æœ¯ç ”ç©¶çš„é“è·¯ä¸Šè¶Šèµ°è¶Šè¿œğŸ›«ğŸ›«ğŸ›«ï¼;ä¸æ–­å–å¾—æ–°çš„çªç ´å’Œæˆå°±ğŸ’ªğŸ’ªğŸ’ªï¼" alt="Typing SVG" /> -->


<div id="footer">
<div id="footer-text">
<br>Page generated 2024-06-30, by <a href="https://qizhuang-qz.github.io/">Zhuang Qi</a>.
</div>
</div>
</div>
</body>
</html>
